# -*- coding: utf-8 -*-
"""CapDec_Finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sl2OesTFRYPRAaE88UpPnOjY0vcJ4Yvy

# Install Dependencies
"""

!pip install transformers
!pip install pycocoevalcap
!pip install git+https://github.com/openai/CLIP.git
!pip install -U sentence-transformers

from google.colab import drive
drive.mount('/content/drive')

"""# Defining and Import Model"""

import gdown
import pickle, json
import requests
import random
import clip
import os
import nltk
from torch import nn
import numpy as np
import torch
import torch.nn.functional as nnf
import sys
from typing import Tuple, List, Union, Optional
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup
from tqdm import tqdm, trange
from google.colab import files
import skimage.io as io
import PIL.Image
from IPython.display import Image
from pycocoevalcap.bleu.bleu import Bleu
import matplotlib.pyplot as plt
from pycocoevalcap.meteor.meteor import Meteor
from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.rouge.rouge import Rouge
from pycocoevalcap.spice.spice import Spice
from sentence_transformers import SentenceTransformer, util

N = type(None)
V = np.array
ARRAY = np.ndarray
ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]
VS = Union[Tuple[V, ...], List[V]]
VN = Union[V, N]
VNS = Union[VS, N]
T = torch.Tensor
TS = Union[Tuple[T, ...], List[T]]
TN = Optional[T]
TNS = Union[Tuple[TN, ...], List[TN]]
TSN = Optional[TS]
TA = Union[T, ARRAY]


D = torch.device
CPU = torch.device('cpu')


def get_device(device_id: int) -> D:
    if not torch.cuda.is_available():
        return CPU
    device_id = min(torch.cuda.device_count() - 1, device_id)
    return torch.device(f'cuda:{device_id}')


CUDA = get_device

class ClipCaptionModel(nn.Module):

    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:
        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)

    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None):
        embedding_text = self.gpt.transformer.wte(tokens)
        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)
        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)
        if labels is not None:
            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)
            labels = torch.cat((dummy_token, tokens), dim=1)
        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)
        return out

    def __init__(self):
        super(ClipCaptionModel, self).__init__()
        self.prefix_length = 40
        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')
        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]
        self.clip_project = TransformerMapper(640, self.gpt_embedding_size, 40, 40, 8)



class MLP(nn.Module):

    def forward(self, x: T) -> T:
        return self.model(x)

    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):
        super(MLP, self).__init__()
        layers = []
        for i in range(len(sizes) -1):
            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))
            if i < len(sizes) - 2:
                layers.append(act())
        self.model = nn.Sequential(*layers)



class ClipCaptionPrefix(ClipCaptionModel):

    def parameters(self, recurse: bool = True):
        return self.clip_project.parameters()

    def train(self, mode: bool = True):
        super(ClipCaptionPrefix, self).train(mode)
        self.gpt.eval()
        return self

def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,
                  entry_length=67, temperature=1., stop_token: str = '.'):

    model.eval()
    stop_token_index = tokenizer.encode(stop_token)[0]
    tokens = None
    scores = None
    device = next(model.parameters()).device
    seq_lengths = torch.ones(beam_size, device=device)
    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)
    with torch.no_grad():
        if embed is not None:
            generated = embed
        else:
            if tokens is None:
                tokens = torch.tensor(tokenizer.encode(prompt))
                tokens = tokens.unsqueeze(0).to(device)
                generated = model.gpt.transformer.wte(tokens)
        for i in range(entry_length):
            outputs = model.gpt(inputs_embeds=generated)
            logits = outputs.logits
            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)
            logits = logits.softmax(-1).log()
            if scores is None:
                scores, next_tokens = logits.topk(beam_size, -1)
                generated = generated.expand(beam_size, *generated.shape[1:])
                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)
                if tokens is None:
                    tokens = next_tokens
                else:
                    tokens = tokens.expand(beam_size, *tokens.shape[1:])
                    tokens = torch.cat((tokens, next_tokens), dim=1)
            else:
                logits[is_stopped] = -float(np.inf)
                logits[is_stopped, 0] = 0
                scores_sum = scores[:, None] + logits
                seq_lengths[~is_stopped] += 1
                scores_sum_average = scores_sum / seq_lengths[:, None]
                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)
                next_tokens_source = next_tokens // scores_sum.shape[1]
                seq_lengths = seq_lengths[next_tokens_source]
                next_tokens = next_tokens % scores_sum.shape[1]
                next_tokens = next_tokens.unsqueeze(1)
                tokens = tokens[next_tokens_source]
                tokens = torch.cat((tokens, next_tokens), dim=1)
                generated = generated[next_tokens_source]
                scores = scores_sum_average * seq_lengths
                is_stopped = is_stopped[next_tokens_source]
            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)
            generated = torch.cat((generated, next_token_embed), dim=1)
            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()
            if is_stopped.all():
                break
    scores = scores / seq_lengths
    output_list = tokens.cpu().numpy()
    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]
    order = scores.argsort(descending=True)
    output_texts = [output_texts[i] for i in order]
    return output_texts


def generate_without_beam(
        model,
        tokenizer,
        tokens=None,
        prompt=None,
        embed=None,
        entry_count=1,
        entry_length=67,  # maximum number of words
        top_p=0.8,
        temperature=1.,
        stop_token: str = '.',
):
    model.eval()
    generated_num = 0
    generated_list = []
    stop_token_index = tokenizer.encode(stop_token)[0]
    filter_value = -float("Inf")
    device = next(model.parameters()).device

    with torch.no_grad():

        for entry_idx in trange(entry_count):
            if embed is not None:
                generated = embed
            else:
                if tokens is None:
                    tokens = torch.tensor(tokenizer.encode(prompt))
                    tokens = tokens.unsqueeze(0).to(device)

                generated = model.gpt.transformer.wte(tokens)

            for i in range(entry_length):

                outputs = model.gpt(inputs_embeds=generated)
                logits = outputs.logits
                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[
                                                    ..., :-1
                                                    ].clone()
                sorted_indices_to_remove[..., 0] = 0

                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                logits[:, indices_to_remove] = filter_value
                next_token = torch.argmax(logits, -1).unsqueeze(0)
                next_token_embed = model.gpt.transformer.wte(next_token)
                if tokens is None:
                    tokens = next_token
                else:
                    tokens = torch.cat((tokens, next_token), dim=1)
                generated = torch.cat((generated, next_token_embed), dim=1)
                if stop_token_index == next_token.item():
                    break

            output_list = list(tokens.squeeze().cpu().numpy())
            output_text = tokenizer.decode(output_list)
            generated_list.append(output_text)

    return generated_list[0]

is_gpu = True #@param {type:"boolean"}
device = CUDA(0) if is_gpu else "cpu"
clip_model, preprocess = clip.load("RN50x4", device=device, jit=False)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

class MlpTransformer(nn.Module):
    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):
        super().__init__()
        out_d = out_d if out_d is not None else in_dim
        self.fc1 = nn.Linear(in_dim, h_dim)
        self.act = act
        self.fc2 = nn.Linear(h_dim, out_d)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x

class MultiHeadAttention(nn.Module):

    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim_self // num_heads
        self.scale = head_dim ** -0.5
        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)
        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)
        self.project = nn.Linear(dim_self, dim_self)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, y=None, mask=None):
        y = y if y is not None else x
        b, n, c = x.shape
        _, m, d = y.shape
        # b n h dh
        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)
        # b m 2 h dh
        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)
        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]
        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale
        if mask is not None:
            if mask.dim() == 2:
                mask = mask.unsqueeze(1)
            attention = attention.masked_fill(mask.unsqueeze(3), float("-inf"))
        attention = attention.softmax(dim=2)
        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)
        out = self.project(out)
        return out, attention


class TransformerLayer(nn.Module):

    def forward_with_attention(self, x, y=None, mask=None):
        x_, attention = self.attn(self.norm1(x), y, mask)
        x = x + x_
        x = x + self.mlp(self.norm2(x))
        return x, attention

    def forward(self, x, y=None, mask=None):
        x = x + self.attn(self.norm1(x), y, mask)[0]
        x = x + self.mlp(self.norm2(x))
        return x

    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,
                 norm_layer: nn.Module = nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim_self)
        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)
        self.norm2 = norm_layer(dim_self)
        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)


class Transformer(nn.Module):

    def forward_with_attention(self, x, y=None, mask=None):
        attentions = []
        for layer in self.layers:
            x, att = layer.forward_with_attention(x, y, mask)
            attentions.append(att)
        return x, attentions

    def forward(self, x, y=None, mask=None):
        for i, layer in enumerate(self.layers):
            if i % 2 == 0 and self.enc_dec: # cross
                x = layer(x, y)
            elif self.enc_dec:  # self
                x = layer(x, x, mask)
            else:  # self or cross
                x = layer(x, y, mask)
        return x

    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,
                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):
        super(Transformer, self).__init__()
        dim_ref = dim_ref if dim_ref is not None else dim_self
        self.enc_dec = enc_dec
        if enc_dec:
            num_layers = num_layers * 2
        layers = []
        for i in range(num_layers):
            if i % 2 == 0 and enc_dec:  # cross
                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))
            elif enc_dec:  # self
                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))
            else:  # self or cross
                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))
        self.layers = nn.ModuleList(layers)


class TransformerMapper(nn.Module):

    def forward(self, x):
        x = self.linear(x).view(x.shape[0], self.clip_length, -1)
        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)
        prefix = torch.cat((x, prefix), dim=1)
        out = self.transformer(prefix)[:, self.clip_length:]
        return out

    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):
        super(TransformerMapper, self).__init__()
        self.clip_length = clip_length
        self.transformer = Transformer(dim_embedding, 8, num_layers)
        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)
        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)

"""# Download Pre-trained Model & Dataset Annotations"""

def download_pretrained_model(file_name, file_id):
  destination_dir = './' # ./content/ in colab
  model_path = None
  url = f'https://drive.google.com/uc?id={file_id}'
  model_path = f'{destination_dir}{file_name}'
  gdown.download(url, model_path, quiet=True)

  model = ClipCaptionModel()
  model.load_state_dict(torch.load(model_path, map_location=CPU), strict=False)
  model = model.eval()
  device = CUDA(0) if is_gpu else "cpu"
  model = model.to(device)
  return model

pre_trained_file_ids = {
  'News_weights015.pt': '1HspquJZxCLckFT9qs0EzdTU6VcOU9Lmj',
  'Shakespeare_weights015.pt': '19F4gwGI2Zt2T0qaH8THNEosKPF2_PbQ_',
}

def download_dataset_annotation():
  file_ids = { # annotations are stored at: https://drive.google.com/drive/folders/1pO55IlZYmvhQz5EkL4t_U5ct55ko6aTm?usp=sharing
    'dataset_coco.json': '1mG5Ikg82xVBSNK_sNeGrDVM-vehbzSCh',
    'dataset_flickr8k.json': '1_-d_1YFqCeita6de-NVdgrX8T00u4_ch',
    'dataset_nocaps.json': '1ufTQZwia7R4qUMFVlvpIj76MAXYTNL-y',
    'dataset_vizwiz.json': '1nQgvVJ2QMmjcwvxmPlMZgq2HniRqsMkY',
    'COCO_2017_test.json': '171pxJ8wZsOUTUa5ecOzdGuF5J0Pji1J0',
    'COCO_2014_test.json': '1ROkbW1FAyHqA4GPGlrrt0o9i4YIBj0Ix',
  }

  destination_dir = './' # ./content/ in colab

  for file_name, file_id in file_ids.items():
    url = f'https://drive.google.com/uc?id={file_id}'
    output = f'{destination_dir}{file_name}'
    gdown.download(url, output, quiet=False)

download_dataset_annotation()

"""# Pre-processing Annotation"""

# get images url
def images_url(json_file, isSubset):
  with open(json_file, 'r') as file:
    data = json.load(file)
  test_images = None
  if isSubset is False:
    test_images = [image for image in data['images'] if image.get('split') == 'test']
  else:
    test_images = [image for image in data['images']]
  return test_images

# pre-processing sentences
def preprocess_sentence(sentence):
  return sentence.replace(",", "").replace(".", "").rstrip()

abcde = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')
fs = "A cat on a table"
ss = "A dog on a table"
fsc = abcde.encode(fs, convert_to_tensor=True)
ssc = abcde.encode(ss, convert_to_tensor=True)
sssss = util.pytorch_cos_sim(fsc, ssc)

"""# Generating Caption and Evaluating Model's Performance"""

def generate_caption(model, dataset_name, url, use_beam_search, TEST_SIZE, test_images):
  output_captions = {}
  save_full_captions = {}
  print(f'Working on {dataset_name}\n')

  if 'file_name' not in test_images[0]:
    replacement = 'filename'
  else: replacement = 'file_name'

  with tqdm(total=TEST_SIZE, desc=f"Generating Captions") as pbar:
    for i in range(TEST_SIZE):
      # print(f'{url}{test_images[i].get(replacement)}')
      image = io.imread(f'{url}{test_images[i].get(replacement)}')
      pil_image = PIL.Image.fromarray(image)

      image = preprocess(pil_image).unsqueeze(0).to(device)
      with torch.no_grad():
        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)
        prefix_embed = model.clip_project(prefix).reshape(1, 40, -1)
      if use_beam_search:
        caption_text = generate_beam(model, tokenizer, embed=prefix_embed)[0]
      else:
        caption_text = generate_without_beam(model, tokenizer, embed=prefix_embed)
      output_captions[i] = [preprocess_sentence(caption_text)]
      save_full_captions[i] = [caption_text]
      pbar.update(1)
    # print(output_captions)
    # print(save_full_captions)
    return output_captions, save_full_captions

def generate_caption2(model, url_or_path, use_beam_search, TEST_SIZE, test_images):
    output_captions = {}
    if 'file_name' not in test_images[0]:
      replacement = 'filename'
    else: replacement = 'file_name'

    for i in range(TEST_SIZE):
        if url_or_path.startswith("http"):

          image = io.imread(f"{url_or_path}{test_images[i].get(replacement)}")
        else:
            image = io.imread(test_images[i])

        pil_image = PIL.Image.fromarray(image)

        image = preprocess(pil_image).unsqueeze(0).to(device)
        with torch.no_grad():
            prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)
            prefix_embed = model.clip_project(prefix).reshape(1, 40, -1)
        if use_beam_search:
            caption_text = generate_beam(model, tokenizer, embed=prefix_embed)[0]
        else:
            caption_text = generate_without_beam(model, tokenizer, embed=prefix_embed)
        output_captions[i] = [caption_text]

        display(pil_image)
        print(f'{caption_text}\n\n')

# get ref captions
def ground_truth_captions(test_images):
  tokens = {}
  for i in range(len(test_images)):
    tokens[i] = []
    for sentence in test_images[i].get('sentences'):
        tokens[i].append(sentence['raw'])
        tokens[i] = [preprocess_sentence(sentence) for sentence in tokens[i]]
  return tokens

nocaps_img_path = 'https://s3.amazonaws.com/nocaps/val/'
coco_img_path = 'http://images.cocodataset.org/val2014/'

class MetricsEvaluation():
  def __init__(self, model_name):
    self.model_name = model_name
    self.metrics_score = {}

  def metrics_calculation(self, reference_sentences, captions_from_model):
    n = 4
    scorer_bleu = Bleu(n)
    #score_meteor = Meteor()
    scorer_cider = Cider()
    scorer_rouge = Rouge()
    scorer_spice = Spice()

    #captions_from_model = {key: [value] for key, value in captions_from_model.items()}

    bleu_score, _ = scorer_bleu.compute_score(reference_sentences, captions_from_model)
    #_meteor_score, _ = score_meteor.compute_score(reference_sentences, captions_from_model)
    cider_score, _ = scorer_cider.compute_score(reference_sentences, captions_from_model)
    rouge_score, _ = scorer_rouge.compute_score(reference_sentences, captions_from_model)
    spice_score, _ = scorer_rouge.compute_score(reference_sentences, captions_from_model)

    scores = {
      'BLEU-1': round(bleu_score[0], 3),
      'BLEU-2': round(bleu_score[1], 3),
      'BLEU-3': round(bleu_score[2], 3),
      'BLEU-4': round(bleu_score[3], 3),
      #'METEOR': round(_meteor_score, 3),
      'CIDEr': round(cider_score, 3),
      'ROUGE': round(rouge_score, 3),
      'SPICE': round(spice_score, 3)
    }
    self.metrics_score = scores


class FineTuneEvaluation():
  def __init__(self, model_name):
    self.model_name = model_name
    self.metrics_score = 'None'
  def cosine_similarity(self, reference_sentences, captions_from_model):
    self.model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')

def visualize_evaluation(score):
  metric_names = list(score.score.keys())
  metric_scores = list(score.score.values())

  plt.figure(figsize=(10, 8))
  plt.bar(metric_names, metric_scores, color='skyblue')
  plt.xlabel('Metrics')
  plt.ylabel('Scores')
  plt.title(f'CapDec with {score.model_name}')
  plt.ylim(0, 1)
  plt.tight_layout()
  plt.show()

def delete_unavailable_images(test_images, url):
  print(f'Since there are some images in Flickr unavailable to open, we will remove them, please patient!')
  new_test_set = []
  with tqdm(total=len(test_images), desc=f"Processing") as pbar:
    for i, image in enumerate(test_images):
      image_url = f'{url}{image.get("filename")}'
      response = requests.head(image_url)
      if response.status_code == 200:
        new_test_set.append(test_images[i])
      pbar.update(1)
  return new_test_set

def exp_evaluation(model, json_file, isSubset, dataset_name, url, title):
  test_images = images_url(json_file, isSubset)
  if 'Flickr' in dataset_name:
    test_images = delete_unavailable_images(test_images, url)

  reference_sentences = ground_truth_captions(test_images)
  captions_from_model, save_full_captions = generate_caption(model, dataset_name, url, True, len(test_images), test_images)

  score = MetricsEvaluation(f"{title} {dataset_name}")
  score.metrics_calculation(reference_sentences, captions_from_model)
  print(f'\nScoring for {score.model_name}\n{score.metrics_score}')
  visualize_evaluation(score.metrics_score)

  return test_images, save_full_captions, score.metrics_score

def finetune_evaluation(model, json_file, isSubset, dataset_name, url, title):
  test_images = images_url(json_file, isSubset)
  if 'Flickr' in dataset_name:
    test_images = delete_unavailable_images(test_images, url)
  test_images = test_images[:5000] if len(test_images) > 5000 else test_images
  captions_from_model, save_full_captions = generate_caption(model, dataset_name, url, True, len(test_images), test_images)

  return test_images, save_full_captions

class ExperimentTest():
  def __init__(self, json_file, isSubset, dataset_name, model_name, url, title):
    self.json_file = json_file
    self.isSubset = isSubset
    self.dataset_name = dataset_name
    self.url = url
    self.title = title,
    self.model = download_pretrained_model(model_name, pre_trained_file_ids[model_name])

  def evaluate(self):
    self.test_images, self.captions_from_model, self.score = exp_evaluation(
        self.model,
        self.json_file,
        self.isSubset,
        self.dataset_name,
        self.url,
        self.title
    )

  def save_result(self, folder):
    image_captioning = []
    for i in range(len(self.test_images)):
      img_data = {
        "url": f'{self.url}{self.test_images[i].get("filename")}',
        "caption": self.captions_from_model[i][0]
      }
      image_captioning.append(img_data)

    data = {
      "noise_variance": self.score if isinstance(self.score, str) else self.score.model_name.split()[0],
      "scores": 'None' if isinstance(self.score, str) else self.score.metrics_score,
      "image_captioning": image_captioning
    }

    directory = f'./drive/MyDrive/CS412-Computer_Vision/{folder}/'
    if not os.path.exists(directory):
      os.makedirs(directory)

    filename = f"{self.dataset_name}.json"
    with open(f'{directory}{filename}', 'w') as file:
      json.dump(data, file, indent=4)

class FineTuneTest(ExperimentTest):
  def __init__(self, json_file, isSubset, dataset_name, model_name, url, title):
    super().__init__(json_file, isSubset, dataset_name, model_name, url, title)
    self.score = title

  def evaluate(self):
    self.test_images, self.captions_from_model = finetune_evaluation(
        self.model,
        self.json_file,
        self.isSubset,
        self.dataset_name,
        self.url,
        self.title
    )

  def save_result(self, folder):
    super().save_result(folder)

  def remove_sensitive(self, caption):
    for i in ['UPDTED', 'www', 'Download', 'Jr', 'per cent', 'Hermia', 'GLOUCESTER']:
      if i in caption:
        return True
    return False

  def scoring(self, pretrain, finetune):
    with open(pretrain, 'r') as file1, open(finetune, 'r') as file2:
      data1 = json.load(file1)
      data2 = json.load(file2)

    pretrain_captions = [item['caption'] for item in data1['image_captioning']]
    finetune_captions = [item['caption'] for item in data2['image_captioning']]
    self.cosim = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')
    res = []
    if (len(pretrain_captions) == len(finetune_captions)):
      with tqdm(total=len(pretrain_captions), desc=f"Scoring") as pbar:
        for caption1, caption2 in zip(pretrain_captions, finetune_captions):
          if '\n' in caption2:
            caption2 = caption2.replace('\n', ' ')
          if self.remove_sensitive(caption2) == False:
            embedding1 = self.cosim.encode(caption1, convert_to_tensor=True)
            embedding2 = self.cosim.encode(caption2, convert_to_tensor=True)

            cosine_similarity = util.pytorch_cos_sim(embedding1, embedding2)
            res.append(cosine_similarity.item())
          else: res.append(0.5)
          pbar.update(1)

    self.score = sum(res) / len(res)
    print(f"\n\nAverage Cosine Similarity: {self.score}")

"""# Fine-tuning

## Path to gg drive
"""

save_path = 'fine-tune_evaluation'
pretrain_path = './drive/MyDrive/CS412-Computer_Vision/evaluation/'
finetune_path = './drive/MyDrive/CS412-Computer_Vision/fine-tune_evaluation/'

"""## COCO 2014"""

COCO_News = FineTuneTest('./dataset_coco.json', False, 'MSCOCO_2014_N', 'News_weights015.pt', coco_img_path, 'Fine-tune 0.015')
COCO_News.evaluate()
COCO_News.save_result(save_path)
COCO_News.scoring(f'{pretrain_path}COCO_test_set.json', f'{finetune_path}{COCO_News.dataset_name}.json')

COCO_Shkp = FineTuneTest('./dataset_coco.json', False, 'MSCOCO_2014_SP', 'Shakespeare_weights015.pt', coco_img_path, 'Fine-tune 0.015')
COCO_Shkp.evaluate()
COCO_Shkp.save_result(save_path)
COCO_Shkp.scoring(f'{pretrain_path}COCO_test_set.json', f'{finetune_path}{COCO_Shkp.dataset_name}.json')

"""## Nocaps"""

Nocaps_News = FineTuneTest('./dataset_nocaps.json', True, 'Nocaps_N', 'News_weights015.pt', nocaps_img_path, 'Fine-tune 0.015')
Nocaps_News.evaluate()
Nocaps_News.save_result(save_path)
Nocaps_News.scoring(f'{pretrain_path}Nocaps_test_set.json', f'{finetune_path}{Nocaps_News.dataset_name}.json')

Nocaps_Shkp = FineTuneTest('./dataset_nocaps.json', True, 'Nocaps_SP', 'Shakespeare_weights015.pt', 'https://s3.amazonaws.com/nocaps/val/', 'Fine-tune 0.015')
Nocaps_Shkp.evaluate()
Nocaps_Shkp.save_result(save_path)
Nocaps_Shkp.scoring(f'{pretrain_path}Nocaps_test_set.json', f'{finetune_path}{Nocaps_Shkp.dataset_name}.json')

"""# Self-Evalutation"""

load_dataset_name = ['./dataset_coco.json']

def finetune_captioning(load_model_name, load_dataset_name, isSubset, img_path):
  test_imgs = images_url(load_dataset_name, isSubset)

  random.shuffle(test_imgs)
  n = 0
  test_imgs = test_imgs[n:n+5]
  model = download_pretrained_model(load_model_name, pre_trained_file_ids[load_model_name])
  generate_caption2(model, img_path, True, len(test_imgs), test_imgs)

"""## news"""

load_model_name = 'News_weights015.pt'
finetune_captioning(load_model_name, load_dataset_name[0], False, coco_img_path)

"""## shakespeare"""

load_model_name = 'Shakespeare_weights015.pt'
finetune_captioning(load_model_name, load_dataset_name[0], False, coco_img_path)

"""## own images"""

device_or_url = 'Device' #@param ["Device", "URL"]
load_model_name = 'News_weights015.pt' #@param ["News_weights015.pt", "Shakespeare_weights015.pt"]
noimage = 0
my_images = []

while True:
  if device_or_url == 'Device':
    print(f"\nUpload image {noimage + 1} or cancel")
    uploaded = files.upload()
    if not uploaded:
      own_image = ''
      break
    elif len(uploaded) == 1:
      own_image = list(uploaded.keys())[0]
      my_images.append(own_image)
      noimage = noimage + 1
    else:
      print('Please upload an image!')
  else:
    url = input(f'(0 to exit) URL of image {noimage + 1}: ')
    if url == '0':
      break
    my_images.append(url)
    noimage = noimage + 1


generate_caption2(download_pretrained_model(load_model_name, pre_trained_file_ids[load_model_name]), '', False, len(my_images), my_images)